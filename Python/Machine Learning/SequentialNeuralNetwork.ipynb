{"cells":[{"cell_type":"markdown","metadata":{"id":"rrMWcItfOvWo"},"source":["# Sequential Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lB-6gCtnOvWs"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","np.__version__"]},{"cell_type":"markdown","metadata":{"id":"J5Y9S1QcOvWv"},"source":["Explore dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1y35IPQOvWy"},"outputs":[],"source":["df = pd.read_csv(\"./data/iris.data\",header=None)\n","names = [\"sepal_length\", \"sepal_width\",\"petal_length\", \"petal_width\", \"class\"]\n","df.columns = names\n","print(df.head())\n","print(df.info())\n","print(df.describe())"]},{"cell_type":"markdown","metadata":{"id":"pHL2iDULOvWz"},"source":["Determine features and target values and Standardize the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fokD0cvzOvW1"},"outputs":[],"source":["x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n","print(x_values.head())\n","#transform your data such that its distribution will have a mean value 0 and standard deviation of 1\n","standardise = StandardScaler()\n","x_values = standardise.fit_transform(x_values)\n","x_values_df = pd.DataFrame(x_values)"]},{"cell_type":"markdown","metadata":{"id":"qMGwENbDOvW3"},"source":["Compare this standardized dataset to the original one"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVXI1IVJOvW4"},"outputs":[],"source":["print(x_values_df.head())"]},{"cell_type":"markdown","metadata":{"id":"8pog-Qe_OvW6"},"source":["Use the .describe function to find out  current mean and std value of the standardized dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlPnqifJOvW8"},"outputs":[],"source":["print(x_values_df.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdMpGwfNOvW_"},"outputs":[],"source":["import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import Dense\n","tf.__version__"]},{"cell_type":"markdown","metadata":{"id":"ASsRV8X5OvXe"},"source":["# Building the neural network (sequential, dense)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfw2sq8DOvXj"},"outputs":[],"source":["# Initialise the neural network as model\n","model4 = Sequential()\n","\n","# Add the first hidden layer with 6 nodes.\n","model4.add(Dense(6,input_dim=4,activation='relu'))\n","\n","# Add the hidden layer with 6 nodes.\n","model4.add(Dense(6,activation='relu'))\n","\n","# Add the output layer with 3 nodes. model4.add(Dense(3,activation='softmax'))\n","\n","# Compile the model together. T\n","model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Train model\n","model4.fit(x_values,y_values,epochs=200,shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"SMBU0dPdOvXo"},"source":["Keep 25 percent of the dataset as the test/validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lcGWp2aOvXp"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Extract out original x_values from the dataframe df\n","x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n","\n","# Test_size=0.25\n","x_train, x_test, y_train, y_test = train_test_split(x_values,y_values,test_size=0.25,random_state=10)\n","\n","# Check the number of rows\n","print(\"Number of rows in x_train:\", x_train.shape[0])\n","print(\"Number of rows in x_test:\", x_test.shape[0])\n","print(\"Number of rows in y_train:\", y_train.shape[0])\n","print(\"Number of rows in y_test:\", y_test.shape[0])\n","\n","# standardiser\n","standardise = StandardScaler()\n","\n","# Standardise the x_train values using .fit_transform\n","x_train = standardise.fit_transform(x_train)\n","\n","# Standardise the x_test values using .transform\n","x_test = standardise.transform(x_test)"]},{"cell_type":"markdown","metadata":{"id":"Y8jEtke1OvXu"},"source":["Train neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVM3ru-2OvXv"},"outputs":[],"source":["# Initialise the neural network as model\n","model_val = Sequential()\n","\n","# Add the first hidden layer with 6 nodes\n","model_val.add(Dense(6,input_dim=4,activation='relu'))\n","\n","# Add the hidden layer with 6 nodes.\n","model_val.add(Dense(6,activation='relu'))\n","\n","# Add the output layer with 3 nodes\n","model_val.add(Dense(3,activation='softmax'))\n","\n","# Compile the model\n","model_val.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Model summary\n","print(model_val.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUwDobfSOvXx"},"outputs":[],"source":["# Train model with x_values and y_values\n","model_val.fit(x_train,y_train,epochs=50,shuffle=True,validation_data=(x_test,y_test))"]},{"cell_type":"markdown","metadata":{"id":"X6m928voOvX0"},"source":["Make predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYcW8j6NOvX1"},"outputs":[],"source":["#load data\n","df2 = pd.read_csv(\"./iris_predict.data\",header=None)\n","names = [\"sepal_length\", \"sepal_width\",\"petal_length\", \"petal_width\"]\n","df2.columns = names\n","\n","x_new = df2[['sepal_length','sepal_width','petal_length','petal_width']]\n","x_new_scale = standardise.transform(x_new)\n","y_new = model_val.predict(x_new_scale)\n","\n","flower_types = []\n","for ii in range(0,y_new.shape[0]):\n","    flower_types.append(np.argmax(y_new[ii,:]))\n","\n","print(flower_types)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
